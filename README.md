# ğŸ¡ Kaggle Housing Price Prediction - Top 2.86% Achievement

https://www.kaggle.com/competitions/home-data-for-ml-course

## ğŸ‘¥ Team
- ğŸ‘¨â€ğŸ’» Nguyen Vu Trong Nhan
- ğŸ‘¨â€ğŸ’» Duong Hoang Anh Khoa
- ğŸ‘©â€ğŸ’» Seokyung Kim
- ğŸ‘©â€ğŸ’» Shirin Shujaa
- ğŸ‘¨â€ğŸ’» Ngo Ngoc Thinh

## ğŸ“Œ Introduction

This project is a submission for the Kaggle Housing Prices competition, where our team secured a top **2.86% ranking globally**. The goal of the competition is to predict house sale prices based on **81** explanatory variables describing various aspects of residential homes in Ames, Iowa. ğŸ ğŸ“Š

## ğŸš€ Getting Started

## ğŸ“– Competition Description

Real estate pricing is influenced by a wide range of factors beyond simple metrics like the number of bedrooms or lot size. This competition provides a dataset containing **81** **features**, capturing nearly every aspect of a home's characteristics. Participants are required to build predictive models to estimate home prices. ğŸ’°ğŸ“‰

### ğŸ›  Skills Practiced

- ğŸ” **Feature Engineering**: Identifying and constructing new variables to improve model performance.
- ğŸ“ˆ **Advanced Regression Techniques**: Implementing methods such as **random forest**, **gradient boosting**, and **stacking models** to achieve high accuracy.

## ğŸ“‚ Dataset and File Descriptions

The dataset consists of the following files:

- ğŸ“„ `train.csv`: The training dataset containing house features and sale prices.
- ğŸ“„ `test.csv`: The test dataset containing house features (without sale prices).
- ğŸ“„ `sample_submission.csv`: A sample submission file demonstrating the required format.
- ğŸ“„ `data_description.txt`: A file explaining the meaning of each column in the dataset.

### ğŸ¯ Target Variable

- **SalePrice**: The final sale price of a home (measured in USD). ğŸ’µ

### ğŸ”‘ Key Features

- ğŸ“ **LotFrontage**: Linear feet of street connected to property
- ğŸ† **OverallQual**: Overall material and finish quality
- ğŸ— **YearBuilt**: Year the house was built
- ğŸ  **TotalBsmtSF**: Total square feet of basement area
- ğŸ“ **GrLivArea**: Above-grade (ground) living area in square feet
- ğŸš— **GarageCars**: Number of cars that fit in the garage

## ğŸ‹ï¸â€â™‚ï¸ Model Training Approach

### ğŸ§¹ Data Preprocessing

1. ğŸ•µï¸â€â™‚ï¸ **Handling Missing Values**: Filling missing values using mean, median, mode, and custom logic (e.g., grouping by neighborhood).
2. âœ‚ **Outlier Removal**: Using IQR (Interquartile Range) to filter extreme values.
3. ğŸ›  **Feature Engineering**:
   - Created new features such as `HouseAge`, `TotalBathrooms`, and `TotalSF`.
   - Applied **K-Means Clustering** on the `Neighborhood` feature.
4. ğŸ­ **Encoding Categorical Variables**:
   - **Label Encoding** for ordinal categorical features.
   - **One-Hot Encoding** for nominal categorical features.
5. ğŸ“ **Feature Scaling**:
   - Used **StandardScaler** for numerical features.
   - Applied **log transformation** to the `SalePrice` variable to reduce skewness.

### ğŸ† Model Selection and Hyperparameter Tuning

We experimented with multiple models and fine-tuned their hyperparameters using **GridSearchCV** and **K-Fold Cross-Validation (k=5)**. The following models were used:

#### ğŸ¤– **Trained Models**

1. ğŸ“Š **Linear Regression**
2. ğŸŒ² **Random Forest Regressor**
3. ğŸš€ **XGBoost Regressor**
4. ğŸ± **CatBoost Regressor** *(Best Performing Model)*
5. ğŸŒ¿ **LightGBM Regressor**
6. ğŸ”— **Stacked Model (Ensemble of Best Models)**

#### ğŸ”— **Stacking Model Implementation**

To further improve performance, we used a **stacking regressor**, which combines predictions from multiple models to create a meta-model:

```python
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Ridge

stacked_model = StackingRegressor(
    estimators=[
        ('xgb', XGBRegressor(n_estimators=500, learning_rate=0.05)),
        ('catboost', CatBoostRegressor(iterations=500, learning_rate=0.05, verbose=0)),
        ('lgbm', LGBMRegressor(n_estimators=500, learning_rate=0.05))
    ],
    final_estimator=Ridge(alpha=1.0)
)
```

### ğŸ“Š Performance Evaluation

Models were evaluated using **Root Mean Squared Error (RMSE)**:

- âœ… RMSE measures the error between the predicted and actual sale prices, with lower values indicating better performance.
- âœ… The metric is applied to the **log-transformed SalePrice** to ensure errors in high-value and low-value houses are treated equally.

### ğŸ† Best Model Results

- ğŸ¥‡ **Stacking Regressor achieved the best RMSE on the test set**, leading to our final submission.
- ğŸ¥ˆ The **CatBoost model** also provided competitive results and was used as a backup submission.

## ğŸ“‘ Submission Format

The submission file (`submission.csv`) must follow the format below:

```
Id,SalePrice
1461,169000.1
1462,187724.1233
1463,175221.0
```

## ğŸ”‘ Key Takeaways

- ğŸ¯ **Feature Engineering** is crucial for achieving high performance.
- ğŸ¤– **Ensemble Models** (Stacking, CatBoost, LightGBM) significantly improve accuracy.
- âš™ **Hyperparameter Tuning** plays a major role in model optimization.
- ğŸ  **Handling Missing Data Correctly** prevents biases in the model.

## ğŸ™Œ Acknowledgments

This project is based on the **Ames Housing Dataset**, compiled by **Dean De Cock** for data science education. Special thanks to the Kaggle community for sharing valuable insights and methodologies. ğŸ’¡

## ğŸ”® Future Improvements

- ğŸ¤– Experimenting with **Neural Networks (MLPs)** for regression.
- ğŸ† **Automated Feature Selection** using Lasso regression.
- ğŸ¯ Implementing a **Bayesian Optimization-based Hyperparameter Tuning** instead of GridSearchCV.

ğŸš€ *Happy Coding!*
